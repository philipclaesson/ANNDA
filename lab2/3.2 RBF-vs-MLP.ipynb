{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here will will evaluate the effect of the RBF by comparing with the multi layer perceptron of lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Support functions for the evaluation\n",
    "\n",
    "def getTrainSet(func = 'sin2x', stepSize = 0.1, noise = True):\n",
    "    ## Returns an 2 x N array of a training set\n",
    "    # Row 0 = inputs and row 1 = targets¨\n",
    "    # stepSize is taken as input, range is fixed to 0 --> 2pi. \n",
    "    N = int(np.floor(2 * np.pi/stepSize)) #Number of datapoints\n",
    "    \n",
    "    train = np.zeros((2,N))\n",
    "    inputs = np.arange(0, N)\n",
    "    np.random.shuffle(inputs)\n",
    "    \n",
    "    if (noise):\n",
    "        noise = 1\n",
    "    else: \n",
    "        noise = 0\n",
    "    \n",
    "    for step, i in enumerate(inputs):\n",
    "        train[0, i] = step*stepSize # Input: will be for example 0, 0.1, 0.2 .. 2pi etc.. \n",
    "        if (func == 'sin2x'): \n",
    "            train[1, i] = np.sin(2*step*stepSize) + noise * np.random.normal(0, np.sqrt(0.1)) # Target: for example sin(2*0), sin(2*0.1) .. sin(2*2pi) etc..\n",
    "        elif (func == 'step2x'): \n",
    "            train[1, i] = np.sign(np.sin(2*step*stepSize)) + noise * np.random.normal(0, np.sqrt(0.1)) # Target: for example sin(2*0), sin(2*0.1) .. sin(2*2pi) etc..\n",
    "\n",
    "    return train.T\n",
    "\n",
    "def getTestSet(func = 'sin2x', stepSize = 0.1):\n",
    "    ## Returns an 2 x N array of a training set\n",
    "    # Row 0 = inputs and row 1 = targets¨\n",
    "    # stepSize is taken as input, range is fixed to 0 --> 2pi. \n",
    "    N = int(np.floor(2 * np.pi/stepSize)) #Number of datapoints\n",
    "    \n",
    "    test = np.zeros((2,N))\n",
    "\n",
    "    for step in range(N):\n",
    "        test[0, step] = step*stepSize+0.05 # Input: will be for example 0.05, 0.15, 0.25 .. 2pi´0.05 etc.. \n",
    "        if (func == 'sin2x'): \n",
    "            test[1, step] = np.sin(2*step*stepSize+0.05) # Target: for example sin(2*0), sin(2*0.1) .. sin(2*2pi) etc..\n",
    "        elif (func == 'step2x'): \n",
    "            test[1, step] = np.sign(np.sin(2*step*stepSize+0.05)) # Target: for example sin(2*0), sin(2*0.1) .. sin(2*2pi) etc..\n",
    "\n",
    "    return test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBF:\n",
    "    def __init__(self, n = 12, variance = 0.1, maxinput = 6.28, learning_rate = 0.1):\n",
    "        self.n = n # the number of nodes\n",
    "        self.variance = 0.1 ## Same variance for all nodes\n",
    "        # self.units = np.random.rand(1, n) * maxinput # random unit position in the input space\n",
    "        # self.units = np.array([0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6]) # unit positions \n",
    "        \n",
    "        self.units = np.arange(0, maxinput, maxinput/(n-1))\n",
    "        self.units = np.append(self.units, 6.14)\n",
    "        \n",
    "        ## One node in each max and minimum\n",
    "        # self.units = np.arange(0, maxinput, 3.14/2)\n",
    "        # print(\"Units: \")\n",
    "        # print(self.units)\n",
    "        \n",
    "        self.n = self.units.shape[0]\n",
    "        self.w = np.random.rand(1,self.n).T\n",
    "        self.lr = learning_rate\n",
    "        # print(\"Initiated weights, shape: \")\n",
    "        # print(self.w.shape)\n",
    "    \n",
    "    def error(self, f_approx, f):\n",
    "        # the average error (with direction +/-)\n",
    "        if not (f_approx.shape == f.shape): \n",
    "            raise Exception('f_approx and f shapes mismatch. f_approx: {}, f: {} '.format(f_approx.shape, f.shape))\n",
    "\n",
    "        # error = (phi(x) - target)^2\n",
    "        return np.average(f_approx - f)\n",
    "    \n",
    "    def res_error(self, f_approx, f):\n",
    "        # the residual error (absolute)\n",
    "        if not (f_approx.shape == f.shape): \n",
    "            raise Exception('f_approx and f shapes mismatch. f_approx: {}, f: {} '.format(f_approx.shape, f.shape))\n",
    "\n",
    "        # error = (phi(x) - target)^2\n",
    "        return np.average(np.abs(f_approx - f))            \n",
    "    \n",
    "    def predict(self, inp):\n",
    "        # takes an input inp and returns predictions\n",
    "        # do f^ = phi(x) * w.T\n",
    "        x = inp.dot(np.ones((1, self.n))) # inp x 1 * 1 x n --> inp x n\n",
    "        \n",
    "        # print(\"n x 1:\")\n",
    "        # print(self.w.shape)\n",
    "        \n",
    "        f_approx = self.phi_matrix(x).dot(self.w)\n",
    "        return f_approx\n",
    "    \n",
    "    def fit_lsq(self, inp, f):\n",
    "        # x is the input. Shape: inputs x 1\n",
    "        # f is the true value of the function (aka the target), same shape\n",
    "        if not (inp.shape == f.shape): \n",
    "            raise Exception('inp and f shapes mismatch. inp: {}, f: {} '.format(inp.shape, f.shape))\n",
    "    \n",
    "\n",
    "        # We want x to be a matrix with shape: inputs x neurons\n",
    "        x = inp.dot(np.ones((1, self.n))) # inp x 1 * 1 x n --> inp x n\n",
    "        \n",
    "        \n",
    "        # get phi(x)\n",
    "        self.phi_x = self.phi_matrix(x)\n",
    "\n",
    "        # we obtain the w that minimizes the error by solving: \n",
    "        # phi(x).T * phi(x) * w = phi(x).T * f\n",
    "        # --> w = (phi(x).T * phi(x))^-1 * phi(x).T * f        \n",
    "        self.w = np.linalg.inv(self.phi_x.T.dot(self.phi_x)).dot(self.phi_x.T).dot(f)\n",
    "        \n",
    "    \n",
    "    def fit_delta(self, inp, f):\n",
    "        # x is the input. Shape: inputs x 1\n",
    "        # f is the true value of the function (aka the target), same shape\n",
    "        if not (inp.shape == f.shape): \n",
    "            raise Exception('inp and f shapes mismatch. inp: {}, f: {} '.format(inp.shape, f.shape))\n",
    "        \n",
    "        x = inp.dot(np.ones((1, self.n))) # inp x 1 * 1 x n --> inp x n\n",
    "        \n",
    "        # compute phi(x)\n",
    "        self.phi_x = self.phi_matrix(x)\n",
    "        \n",
    "        # make a prediction\n",
    "        f_approx = self.predict(inp)\n",
    "        \n",
    "        # compute the error\n",
    "        e = self.error(f_approx, f)\n",
    "        \n",
    "        # find delta w\n",
    "        self.dw = -1 * self.lr * e * self.phi_x.T\n",
    "        \n",
    "        #print(\"dw shape\")\n",
    "        #print(dw.shape)\n",
    "\n",
    "        \n",
    "        #update weights\n",
    "        self.w += self.dw\n",
    "\n",
    "    def phi(self, x, i): \n",
    "        return np.exp((-(np.linalg.norm(x-i))**2)/(2*np.square(self.variance)))\n",
    "    \n",
    "    \n",
    "    def phi_matrix(self, x):\n",
    "        # number of inputs (m) (rows) x self.n (n) (columns)\n",
    "        # print(\"phi_matrix input size:\" + str(x.shape))\n",
    "        # this is a slow and stupid way of computing phi(x)\n",
    "        for m in range(x.shape[0]): # m\n",
    "            for n in range(x.shape[1]): # n\n",
    "                res = self.phi(x = x[m, n], i = self.units[n])\n",
    "        #        print(\"phi({}, {}): {}\".format(x[row, col], self.units[0, row], res))\n",
    "                x[m, n] = res\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function = 'sin2x'\n",
    "\n",
    "# Train model\n",
    "trainset = getTrainSet(function, noise = False)\n",
    "\n",
    "## Test Model\n",
    "# Get test set\n",
    "testset = getTestSet(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def batch(n, trainset, testset, variance = 0.1):\n",
    "    model = RBF(n, variance = 0.1)\n",
    "\n",
    "    \n",
    "    # fit model\n",
    "    model.fit_lsq(trainset[:, 0][np.newaxis,:].T, trainset[:, 1][np.newaxis,:].T)\n",
    "\n",
    "\n",
    "    \n",
    "    ## Test Model\n",
    "    # Make predictions\n",
    "    pred = model.predict(testset[:, 0][np.newaxis,:].T)\n",
    "    # test error\n",
    "    e = model.res_error(pred, testset[:, 1][np.newaxis,:].T)\n",
    "\n",
    "    # print(\"Absolute residual error: {}\".format(e))\n",
    "    return round(e, 3)\n",
    "\n",
    "def delta(n, trainset, testset, variance = 0.1, learning_rate = 0.1, epochs = 20):\n",
    "    model = RBF(n, variance = variance, learning_rate = learning_rate)\n",
    "    \n",
    "    #fit model\n",
    "    for epoch in range(epochs): \n",
    "        for tupl in trainset: \n",
    "            sample = np.array([tupl[0]])[np.newaxis,:].T\n",
    "            target = np.array([tupl[1]])[np.newaxis,:].T\n",
    "            model.fit_delta(sample, target)\n",
    "        # Make predictions\n",
    "        pred = model.predict(testset[:, 0][np.newaxis,:].T)\n",
    "        # test error\n",
    "        e = model.res_error(pred, testset[:, 1][np.newaxis,:].T)\n",
    "        # if (epoch % 5 == 0):\n",
    "            # print(\"Epoch {}, residual error: {}, dw: {}\".format(epoch, e, np.average(model.dw)))\n",
    "        # print(\"Epoch {}, residual error: {}, dw: {} \".format(epoch, e, np.average(model.dw)))\n",
    "        return round(e, 2)\n",
    "units = 60\n",
    "width = 0.01\n",
    "batch(units, trainset, testset, width)\n",
    "delta(units, trainset, testset, width, 0.5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    def __init__(self, inputs, no_nodes1, no_nodes2, eta, epochs):\n",
    "        self.no_nodes_first_layer = no_nodes1\n",
    "        self.no_nodes_second_layer = no_nodes2  \n",
    "        self.no_input_nodes = inputs\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.shape_y = 0\n",
    "        self.number_of_layers = 2\n",
    "        \n",
    "        self.train_zero_losses = []\n",
    "        self.train_ms_errors = []\n",
    "        self.test_zero_losses = []\n",
    "        self.test_ms_errors = []\n",
    "        \n",
    "        self.trained_weights = []\n",
    "        \n",
    "    def phi_function(self, x):\n",
    "        return (2 / (1 + np.exp(-x))) - 1\n",
    "    \n",
    "    def phi_prime(self, x):\n",
    "        return (1 + x)*(1 - x) / 2\n",
    "    \n",
    "    def init_weights(self, no_inputs, no_neurons):\n",
    "        #np.random.seed(42)\n",
    "        return np.random.normal(0, 1, size=(no_inputs + 1, no_neurons))\n",
    "    \n",
    "    def examine(self, x):\n",
    "        w = self.trained_weights[0]\n",
    "        v = self.trained_weights[1]\n",
    "        hout,_ = self.forward_pass(v, w, x) \n",
    "        return hout[:,:3]\n",
    "    \n",
    "    def forward_pass(self, v, w, x):\n",
    "        bias = np.ones([1, len(x)])\n",
    "        x = np.column_stack([x, bias.T])\n",
    "        hin = np.dot(x, w)\n",
    "        hout = self.phi_function(hin)\n",
    "        hout = np.column_stack([hout, bias.T])\n",
    "        \n",
    "        oin = np.dot(hout, v)\n",
    "        oout = self.phi_function(oin)\n",
    "        \n",
    "        return hout, oout\n",
    "    \n",
    "    def backprop(self, v, t, oout, hout):\n",
    "        t = t.reshape(t.shape[0],self.shape_y)        \n",
    "        delta_o = np.multiply((oout - t), self.phi_prime(oout))  \n",
    "        delta_h = np.dot(delta_o, v.T) * self.phi_prime(hout)\n",
    "        delta_h = delta_h[:, :self.no_nodes_first_layer]\n",
    "        return delta_h, delta_o\n",
    "    \n",
    "    def weight_update(self, x, hout, v, w, dv, dw, delta_h, delta_o):\n",
    "        alpha = 0.9\n",
    "        \n",
    "        bias = np.ones(len(x))\n",
    "        x = np.column_stack([x, bias.T])\n",
    "      \n",
    "        dw = (dw * alpha) - np.array(np.dot(x.T, delta_h)) * (1-alpha)\n",
    "        dv = (dv * alpha) - np.array(np.dot(hout.T, delta_o)) * (1-alpha)\n",
    "        \n",
    "        w = w + dw * self.eta\n",
    "        v = v + dv * self.eta \n",
    "        return (w, v, dw, dv)\n",
    "      \n",
    "    def test_accuracy(self, predictions, targets):\n",
    "        # mean squared error\n",
    "        mse = mean_squared_error(targets, predictions)\n",
    "        \n",
    "        # accuracy\n",
    "        for i in range(len(predictions)):\n",
    "            for j in range(len(predictions[i])):\n",
    "                if predictions[i, j] >= 0:\n",
    "                    predictions[i, j] = 1\n",
    "                else:\n",
    "                    predictions[i, j] = -1\n",
    "        zero_one = zero_one_loss(targets, predictions, normalize=True)\n",
    "        \n",
    "        return mse, zero_one\n",
    "        \n",
    "    def batch_learn(self, x_train, y_train, x_test, y_test):\n",
    "        self.shape_y = int(y_train.size/len(y_train))\n",
    "        w = self.init_weights(self.no_input_nodes, self.no_nodes_first_layer)\n",
    "        v = self.init_weights(self.no_nodes_first_layer, self.no_nodes_second_layer)\n",
    "        dw = 0\n",
    "        dv = 0\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            hout, oout = self.forward_pass(v, w, x_train)\n",
    "            delta_h, delta_o = self.backprop(v, y_train, oout, hout)\n",
    "\n",
    "            w, v, dw, dv = self.weight_update(x_train, hout, v, w, dv, dw, delta_h, delta_o)\n",
    "            \n",
    "            _, train_predictions = self.forward_pass(v, w, x_train)\n",
    "            train_mse, train_zero_one = self.test_accuracy(train_predictions, y_train)\n",
    "            \n",
    "            _, test_predictions = self.forward_pass(v, w, x_test)\n",
    "            test_mse, test_zero_one = self.test_accuracy(test_predictions, y_test)\n",
    "            \n",
    "            self.train_zero_losses.append(train_zero_one)\n",
    "            self.train_ms_errors.append(train_mse)\n",
    "            self.test_zero_losses.append(test_zero_one)\n",
    "            self.test_ms_errors.append(test_mse)\n",
    "        self.trained_weights.append(w)\n",
    "        self.trained_weights.append(v)\n",
    "            \n",
    "    def seq_learn(self, x_train, y_train, x_test, y_test):\n",
    "        w = self.init_weights(self.no_input_nodes, self.no_nodes_first_layer)\n",
    "        v = self.init_weights(self.no_nodes_first_layer, self.no_nodes_second_layer)\n",
    "        dw = 0\n",
    "        dv = 0\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            for j in range(len(X_train)):\n",
    "                x = X_train[j,:]\n",
    "                x = np.expand_dims(x, axis=1).T\n",
    "                y = y_train[j]\n",
    "                \n",
    "                hout, oout = self.forward_pass(v, w, x)\n",
    "                delta_h, delta_o = self.backprop(v, y, oout, hout)\n",
    "\n",
    "                w, v, dw, dv = self.weight_update(x, hout, v, w, dv, dw, delta_h, delta_o)\n",
    "\n",
    "            _, train_predictions = self.forward_pass(v, w, x_train)\n",
    "            train_mse, train_zero_one = self.test_accuracy(train_predictions, y_train)\n",
    "            \n",
    "            _, test_predictions = self.forward_pass(v, w, x_test)\n",
    "            test_mse, test_zero_one = self.test_accuracy(test_predictions, y_test)\n",
    "            \n",
    "            self.train_zero_losses.append(train_zero_one)\n",
    "            self.train_ms_errors.append(train_mse)\n",
    "            self.test_zero_losses.append(test_zero_one)\n",
    "            self.test_ms_errors.append(test_mse)\n",
    "            \n",
    "    def plot_errors(self):\n",
    "        f,axarr = plt.subplots(2,sharex=True)\n",
    "        axarr[0].plot(range(self.epochs),self.train_zero_losses, '-',label=\"train\")\n",
    "        axarr[0].plot(range(self.epochs),self.test_zero_losses, '-',label=\"test\")\n",
    "        axarr[0].set_title('Ratio of misclassifications')\n",
    "        \n",
    "        axarr[1].plot(range(self.epochs),self.train_ms_errors, '-',label=\"train\")\n",
    "        axarr[1].plot(range(self.epochs),self.test_ms_errors, '-',label=\"test\")\n",
    "        axarr[1].set_title('Mean squared error')\n",
    "        \n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def final_errors(self):\n",
    "        print(\"Final MSE:\", self.test_ms_errors[-1])\n",
    "        print(\"Final Zero-One Loss:\", self.test_zero_losses[-1])\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Compare RBF with the MLP from lab 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 8\n",
    "first_hidden_layer_nodes = 3\n",
    "second_hidden_layer_nodes = 8\n",
    "eta = 0.01\n",
    "epochs = 750\n",
    "\n",
    "n = NeuralNet(input_nodes, first_hidden_layer_nodes, second_hidden_layer_nodes, eta, epochs)\n",
    "\n",
    "\n",
    "n.batch_learn(X_auto_train, X_auto_train, X_auto_test, X_auto_test)\n",
    "n.plot_errors()\n",
    "n.final_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
